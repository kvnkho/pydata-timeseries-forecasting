{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running on a Dask Cluster (with Coiled)\n",
    "\n",
    "Previously, we expanded each row to the full timeseries to use with the `forecast` function. In practice, we want to be minimizing network data transfer when it comes to distributed computing. \n",
    "\n",
    "The setup shown here is suboptimal because we have a file on a local machine that we are chunking and sending to the Dask workers. All the data is passing through the scheduler. Ideally, the workers should load the partition of data they need. For example, if the data is saved partitioned by `unique_id` already, it will be natural for the workers to load in the partition that is needs to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Because we are passing everything through the scheduler in this scenario, it's a good opportunity to show a technique used to minimize the data transfer. We can send a compressed version. We already have the `combined.parquet` created in the preprocessing. We will just minimize the data footprint before bringing it to the cluster.\n",
    "\n",
    "This means we also need to write logic to handle the decompressing the data passed to the workers. This gives us a good chance to explore how to add custom logic alongside our large scale forecasting. This same approach would be used to persist intermediate artifacts or metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "download_path = os.path.abspath(os.path.join(\".\",\"..\",\"data\",\"m5-forecasting-accuracy.zip\"))\n",
    "unzipped_path = os.path.abspath(os.path.join(\".\",\"..\",\"data\",\"m5-forecasting-accuracy-unzipped\"))\n",
    "\n",
    "# Read in the data\n",
    "INPUT_DIR = unzipped_path\n",
    "WORKING_DIR = os.path.join(unzipped_path, \"..\", \"working\")\n",
    "combined = pd.read_parquet(f'{WORKING_DIR}/combined.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing Data Footprint\n",
    "\n",
    "To minimize the data footprint, we don't need to repeat the string data like `store_id` and `item_id` 2000 times (once for each day). These are also especially big because they are strings. Instead, we'll pack all of the information into one row so only that row gets sent through the scheduler. \n",
    "\n",
    "The function below will put all of the prices and sales volume as a list. No need to worry about the fact that the output is a `List[Dict[str,Any]]`. Similar to what was shown in the previous sections, Fugue will handle the conversion for us later when we apply this function on a Pandas DataFrame by reading the type hints.\n",
    "\n",
    "We'll just review what the `combined` DataFrame looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2013-10-01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>11336</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2013-10-04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>11336</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2013-09-29</td>\n",
       "      <td>0.01</td>\n",
       "      <td>11336</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2013-10-02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>11336</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2013-09-28</td>\n",
       "      <td>0.01</td>\n",
       "      <td>11336</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       unique_id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id         ds     y  wm_yr_wk  sell_price  \n",
       "0       CA 2013-10-01  2.00     11336        8.26  \n",
       "1       CA 2013-10-04  0.01     11336        8.26  \n",
       "2       CA 2013-09-29  0.01     11336        8.26  \n",
       "3       CA 2013-10-02  0.01     11336        8.26  \n",
       "4       CA 2013-09-28  0.01     11336        8.26  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall we just kept the hierarchical columns for the previous section. We actually don't need them for forecasting so we'll drop them so that they don't get sent to the cluster workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'unique_id': 'id1', 'store_id': 'store1', 'item_id': 'item1', 'start_date': datetime.date(2020, 1, 2), 'y': [1, 2, 3], 'prices': [2.2, 3.3, 4.4]}]\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any, Iterable\n",
    "from datetime import date\n",
    "\n",
    "def compress_data(df:pd.DataFrame) -> List[Dict[str,Any]]:\n",
    "    return [dict(unique_id=df.iloc[0][\"unique_id\"],\n",
    "                 store_id=df.iloc[0][\"store_id\"],\n",
    "                 item_id=df.iloc[0]['item_id'],\n",
    "                 start_date=df.iloc[0]['ds'], \n",
    "                 y=df[\"y\"].tolist(),\n",
    "                 prices=df[\"sell_price\"].tolist())]\n",
    "\n",
    "# Testing on sample data\n",
    "df = pd.DataFrame([[\"id1\",\"store1\",\"item1\",date(2020,1,2),1,2.2], \n",
    "                   [\"id1\",\"store1\",\"item1\",date(2020,1,3),2,3.3],\n",
    "                   [\"id1\",\"store1\",\"item1\", date(2020,1,4),3,4.4]], \n",
    "                   columns=[\"unique_id\",\"store_id\", \"item_id\", \"ds\",\"y\",\"sell_price\"])\n",
    "print(compress_data(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above functions will operate on each `store_id` and `item_id` combination. This means we have to use the `partition` argument of the `transform()` function to defining the grouping. This will also work on the distributed backends.\n",
    "\n",
    "The `compress_data()` function assumed the data is sorted. We can apply a `presort` to the `partition` strategy to make sure the data is sorted.\n",
    "\n",
    "This cell can take about 4 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>start_date</th>\n",
       "      <th>y</th>\n",
       "      <th>prices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>[3.0, 0.009999999776482582, 0.0099999997764825...</td>\n",
       "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOODS_1_002_CA_1_evaluation</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>FOODS_1_002</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>[0.009999999776482582, 1.0, 0.0099999997764825...</td>\n",
       "      <td>[7.88, 7.88, 7.88, 7.88, 7.88, 7.88, 7.88, 7.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOODS_1_003_CA_1_evaluation</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>FOODS_1_003</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>[0.009999999776482582, 0.009999999776482582, 0...</td>\n",
       "      <td>[2.88, 2.88, 2.88, 2.88, 2.88, 2.88, 2.88, 2.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOODS_1_004_CA_1_evaluation</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>FOODS_1_004</td>\n",
       "      <td>2012-03-03</td>\n",
       "      <td>[0.009999999776482582, 0.009999999776482582, 0...</td>\n",
       "      <td>[1.78, 1.78, 1.78, 1.78, 1.78, 1.78, 1.78, 1.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOODS_1_005_CA_1_evaluation</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>FOODS_1_005</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>[3.0, 9.0, 3.0, 3.0, 0.009999999776482582, 2.0...</td>\n",
       "      <td>[2.94, 2.94, 2.94, 2.94, 2.94, 2.94, 2.94, 2.9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     unique_id store_id      item_id start_date  \\\n",
       "0  FOODS_1_001_CA_1_evaluation     CA_1  FOODS_1_001 2011-01-29   \n",
       "1  FOODS_1_002_CA_1_evaluation     CA_1  FOODS_1_002 2011-01-29   \n",
       "2  FOODS_1_003_CA_1_evaluation     CA_1  FOODS_1_003 2011-01-29   \n",
       "3  FOODS_1_004_CA_1_evaluation     CA_1  FOODS_1_004 2012-03-03   \n",
       "4  FOODS_1_005_CA_1_evaluation     CA_1  FOODS_1_005 2011-01-29   \n",
       "\n",
       "                                                   y  \\\n",
       "0  [3.0, 0.009999999776482582, 0.0099999997764825...   \n",
       "1  [0.009999999776482582, 1.0, 0.0099999997764825...   \n",
       "2  [0.009999999776482582, 0.009999999776482582, 0...   \n",
       "3  [0.009999999776482582, 0.009999999776482582, 0...   \n",
       "4  [3.0, 9.0, 3.0, 3.0, 0.009999999776482582, 2.0...   \n",
       "\n",
       "                                              prices  \n",
       "0  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...  \n",
       "1  [7.88, 7.88, 7.88, 7.88, 7.88, 7.88, 7.88, 7.8...  \n",
       "2  [2.88, 2.88, 2.88, 2.88, 2.88, 2.88, 2.88, 2.8...  \n",
       "3  [1.78, 1.78, 1.78, 1.78, 1.78, 1.78, 1.78, 1.7...  \n",
       "4  [2.94, 2.94, 2.94, 2.94, 2.94, 2.94, 2.94, 2.9...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fugue import transform\n",
    "\n",
    "compressed = transform(combined, \n",
    "                compress_data, \n",
    "                schema=\"unique_id:str,store_id:str,item_id:str,start_date:datetime,y:[float],prices:[float]\",\n",
    "                partition={\"by\": [\"store_id\", \"item_id\"], \"presort\": \"ds asc\"})\n",
    "compressed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Logic for Each Timeseries\n",
    "\n",
    "Now that the data is compressed, we need to bring it back out to the uncompressed form in order to use `StatsForecast`. This code will execute on the Dask workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_series(df:List[Dict[str,Any]]) -> pd.DataFrame:\n",
    "    row = df[0]\n",
    "    dr = pd.date_range(row[\"start_date\"],periods=len(row[\"y\"]), freq=\"d\")\n",
    "    df = pd.DataFrame({\"y\":row[\"y\"]},index = dr)\n",
    "    df[\"price\"] = pd.Series(row[\"prices\"],index = dr)\n",
    "    df = df.reset_index()\n",
    "    df.columns=[\"ds\", \"y\", \"price\"]\n",
    "    df['unique_id'] = row['unique_id'] \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>price</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ds     y  price                    unique_id\n",
       "0 2011-01-29  3.00    2.0  FOODS_1_001_CA_1_evaluation\n",
       "1 2011-01-30  0.01    2.0  FOODS_1_001_CA_1_evaluation\n",
       "2 2011-01-31  0.01    2.0  FOODS_1_001_CA_1_evaluation\n",
       "3 2011-02-01  1.00    2.0  FOODS_1_001_CA_1_evaluation\n",
       "4 2011-02-02  4.00    2.0  FOODS_1_001_CA_1_evaluation"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = format_series(compressed.iloc[0:1].to_dict(\"records\"))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Cross Validation\n",
    "\n",
    "For timeseries cross validations, we perform the modelling with a sliding window of test sets. This is so we don't predict past data points with future information. Below is a visual representation of this.\n",
    "\n",
    "![img](https://raw.githubusercontent.com/Nixtla/statsforecast/main/nbs/imgs/ChainedWindows.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `StatsForecast()` class also as a `cross_valdation()` method we can use to execute the cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fugue/lib/python3.8/site-packages/statsforecast/core.py:21: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import Naive, CrostonClassic, IMAPA, ADIDA, AutoARIMA\n",
    "\n",
    "def run_model_cv(df: pd.DataFrame):\n",
    "  sf = StatsForecast(df=df, \n",
    "      models=[Naive(),\n",
    "        CrostonClassic(),\n",
    "        IMAPA(),\n",
    "        ADIDA(),\n",
    "        AutoARIMA()\n",
    "    ], \n",
    "      freq=\"D\",\n",
    "      n_jobs=1)\n",
    "\n",
    "  return sf.cross_validation(h=28, n_windows=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this function on our previous `test` DataFrame. The output will be the out-of-fold predictions that we can then use to calculate evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>y</th>\n",
       "      <th>Naive</th>\n",
       "      <th>CrostonClassic</th>\n",
       "      <th>IMAPA</th>\n",
       "      <th>ADIDA</th>\n",
       "      <th>AutoARIMA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FOODS_1_001_CA_1_evaluation</th>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>2016-04-23</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.105067</td>\n",
       "      <td>1.105067</td>\n",
       "      <td>1.105067</td>\n",
       "      <td>1.144669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOODS_1_001_CA_1_evaluation</th>\n",
       "      <td>2016-04-25</td>\n",
       "      <td>2016-04-23</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.105067</td>\n",
       "      <td>1.105067</td>\n",
       "      <td>1.105067</td>\n",
       "      <td>0.941412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOODS_1_001_CA_1_evaluation</th>\n",
       "      <td>2016-04-26</td>\n",
       "      <td>2016-04-23</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.105067</td>\n",
       "      <td>1.105067</td>\n",
       "      <td>1.105067</td>\n",
       "      <td>1.180909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOODS_1_001_CA_1_evaluation</th>\n",
       "      <td>2016-04-27</td>\n",
       "      <td>2016-04-23</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.105067</td>\n",
       "      <td>1.105067</td>\n",
       "      <td>1.105067</td>\n",
       "      <td>0.925206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOODS_1_001_CA_1_evaluation</th>\n",
       "      <td>2016-04-28</td>\n",
       "      <td>2016-04-23</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.105067</td>\n",
       "      <td>1.105067</td>\n",
       "      <td>1.105067</td>\n",
       "      <td>1.167620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    ds     cutoff     y  Naive  \\\n",
       "unique_id                                                        \n",
       "FOODS_1_001_CA_1_evaluation 2016-04-24 2016-04-23  0.01    1.0   \n",
       "FOODS_1_001_CA_1_evaluation 2016-04-25 2016-04-23  2.00    1.0   \n",
       "FOODS_1_001_CA_1_evaluation 2016-04-26 2016-04-23  0.01    1.0   \n",
       "FOODS_1_001_CA_1_evaluation 2016-04-27 2016-04-23  0.01    1.0   \n",
       "FOODS_1_001_CA_1_evaluation 2016-04-28 2016-04-23  0.01    1.0   \n",
       "\n",
       "                             CrostonClassic     IMAPA     ADIDA  AutoARIMA  \n",
       "unique_id                                                                   \n",
       "FOODS_1_001_CA_1_evaluation        1.105067  1.105067  1.105067   1.144669  \n",
       "FOODS_1_001_CA_1_evaluation        1.105067  1.105067  1.105067   0.941412  \n",
       "FOODS_1_001_CA_1_evaluation        1.105067  1.105067  1.105067   1.180909  \n",
       "FOODS_1_001_CA_1_evaluation        1.105067  1.105067  1.105067   0.925206  \n",
       "FOODS_1_001_CA_1_evaluation        1.105067  1.105067  1.105067   1.167620  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = run_model_cv(test)\n",
    "test2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caclulating Error\n",
    "\n",
    "The last part of our pipeline is returning all of the metrics for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def calculate_metrics(cv_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    models = []\n",
    "    metrics = []\n",
    "    for model in [\"Naive\", \"CrostonClassic\", \"IMAPA\", \"ADIDA\", \"AutoARIMA\"]:\n",
    "        models.append(model)\n",
    "        metrics.append(mean_absolute_error(cv_df['y'], cv_df[model]))\n",
    "    out = pd.DataFrame({\"models\": models, \"metric\": metrics})\n",
    "    out['unique_id'] = cv_df.index[0]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this on the out-of-fold predictions from the `test2` DataFrame above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>models</th>\n",
       "      <th>metric</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive</td>\n",
       "      <td>0.923571</td>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CrostonClassic</td>\n",
       "      <td>1.059186</td>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMAPA</td>\n",
       "      <td>1.059186</td>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADIDA</td>\n",
       "      <td>1.059186</td>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AutoARIMA</td>\n",
       "      <td>1.045886</td>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           models    metric                    unique_id\n",
       "0           Naive  0.923571  FOODS_1_001_CA_1_evaluation\n",
       "1  CrostonClassic  1.059186  FOODS_1_001_CA_1_evaluation\n",
       "2           IMAPA  1.059186  FOODS_1_001_CA_1_evaluation\n",
       "3           ADIDA  1.059186  FOODS_1_001_CA_1_evaluation\n",
       "4       AutoARIMA  1.045886  FOODS_1_001_CA_1_evaluation"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_metrics(test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipeline\n",
    "\n",
    "For each timeseries, we will unpack it,run cross-validation, and then calculate metrics. We can wrap this in one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    timeseries = format_series(df.to_dict(\"records\"))\n",
    "    model_cv = run_model_cv(timeseries)\n",
    "    metrics = calculate_metrics(model_cv).reset_index(drop=True)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test it on the first two rows of the `compressed` data using the default Pandas-based engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>models</th>\n",
       "      <th>metric</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive</td>\n",
       "      <td>0.923571</td>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CrostonClassic</td>\n",
       "      <td>1.059186</td>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMAPA</td>\n",
       "      <td>1.059186</td>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADIDA</td>\n",
       "      <td>1.059186</td>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AutoARIMA</td>\n",
       "      <td>1.045886</td>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive</td>\n",
       "      <td>0.673393</td>\n",
       "      <td>FOODS_1_002_CA_1_evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CrostonClassic</td>\n",
       "      <td>0.654460</td>\n",
       "      <td>FOODS_1_002_CA_1_evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>IMAPA</td>\n",
       "      <td>0.654460</td>\n",
       "      <td>FOODS_1_002_CA_1_evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ADIDA</td>\n",
       "      <td>0.654460</td>\n",
       "      <td>FOODS_1_002_CA_1_evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AutoARIMA</td>\n",
       "      <td>0.656686</td>\n",
       "      <td>FOODS_1_002_CA_1_evaluation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           models    metric                    unique_id\n",
       "0           Naive  0.923571  FOODS_1_001_CA_1_evaluation\n",
       "1  CrostonClassic  1.059186  FOODS_1_001_CA_1_evaluation\n",
       "2           IMAPA  1.059186  FOODS_1_001_CA_1_evaluation\n",
       "3           ADIDA  1.059186  FOODS_1_001_CA_1_evaluation\n",
       "4       AutoARIMA  1.045886  FOODS_1_001_CA_1_evaluation\n",
       "5           Naive  0.673393  FOODS_1_002_CA_1_evaluation\n",
       "6  CrostonClassic  0.654460  FOODS_1_002_CA_1_evaluation\n",
       "7           IMAPA  0.654460  FOODS_1_002_CA_1_evaluation\n",
       "8           ADIDA  0.654460  FOODS_1_002_CA_1_evaluation\n",
       "9       AutoARIMA  0.656686  FOODS_1_002_CA_1_evaluation"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(compressed.iloc[0:2], \n",
    "          process, \n",
    "          schema=\"models:str,metric:float,unique_id:str\", \n",
    "          partition={\"by\": \"unique_id\"},)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this works smoothly, we can now bring the execution cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running on a Coiled Cluster\n",
    "\n",
    "[Coiled](https://www.coiled.io/) is one of the easiest ways to get a Dask cluster. You need to be logged in to create a Dask cluster. If you don't want to follow along here, you can still run the code on a `LocalCluster` similar to the previous sections.\n",
    "\n",
    "Coiled requires a software environment which it will use to spin up cluster workers. Below is how to create a software environment for this tutorial.\n",
    "\n",
    "```python\n",
    "import coiled\n",
    "\n",
    "coiled.create_software_environment(\n",
    "    name=\"pydata\",\n",
    "    conda=[\"python=3.8.13\"],\n",
    "    pip=[\"fugue[dask]\", \"statsforecast\", \"scikit-learn\", \"numpy==1.22.4\"],\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a cluster with Coiled after logging in and creating the software environment, you can use the following code insead of the LocalCluster.\n",
    "\n",
    "```python\n",
    "from dask.distributed import Client\n",
    "from coiled import Cluster\n",
    "\n",
    "cluster = Cluster(name=\"pydata\", software=\"pydata\", n_workers=10)\n",
    "client = Client(cluster)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Coiled cluster will give us enough resources to run all of the models for each timeseries. For the sake of demo purposes on LocalCluster, we will just run the full workflow for the first 100 timeseries. Whether you use a LocalCluster or the Coiled cluster, this will return a local Pandas DataFrame.\n",
    "\n",
    "On the local machine, the first 100 timeseries can take roughly 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fugue/lib/python3.8/site-packages/distributed/node.py:183: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 50489 instead\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/fugue/lib/python3.8/site-packages/statsforecast/arima.py:656: RuntimeWarning: overflow encountered in subtract\n",
      "  x -= np.dot(xreg, par[narma + np.arange(ncxreg)])\n",
      "/opt/anaconda3/envs/fugue/lib/python3.8/site-packages/statsforecast/arima.py:656: RuntimeWarning: overflow encountered in subtract\n",
      "  x -= np.dot(xreg, par[narma + np.arange(ncxreg)])\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "cluster = LocalCluster(threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "\n",
    "results = transform(compressed.iloc[0:100], \n",
    "                    process, \n",
    "                    schema=\"models:str,metric:float,unique_id:str\", \n",
    "                    engine=client, \n",
    "                    partition={\"by\": \"unique_id\"})\n",
    "results = results.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have computed all of the metrics, we can sort them and get the best model for each timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoARIMA         40\n",
       "Naive             34\n",
       "CrostonClassic    12\n",
       "ADIDA             10\n",
       "IMAPA              4\n",
       "Name: models, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models = results.sort_values('metric', ascending=True).groupby(\"unique_id\").first()\n",
    "best_models['models'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results show the benefit of trying multiple models for each timeseries. Some of them may be better modelled by `AutoArima` while some of them may be better modelled by other things like `CrostonClassic`. Though the `Naive` model did surprisingly well, it might be due to a lack of tuning with the other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "That wraps up the large scale time series forecasting tutorial. In this tutorial you have seen:\n",
    "\n",
    "1. How to use StatsForecast and scale it to Spark, Dask, or Ray\n",
    "2. How to train multiple models for each individual timeseries\n",
    "3. How to preprocess and iterate on large scale data with Fugue\n",
    "4. Principals of Hierarchical forecasting\n",
    "5. Some practices around distributed computing.\n",
    "\n",
    "Please feel free to reach out for any comments/questions!\n",
    "\n",
    "[Fugue Slack](http://slack.fugue.ai/)\n",
    "\n",
    "[Nixtla Slack](https://join.slack.com/t/nixtlaworkspace/shared_invite/zt-135dssye9-fWTzMpv2WBthq8NK0Yvu6A)\n",
    "\n",
    "My email:\n",
    "kdykho@gmail.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('pydata')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a90540e516f4536bf5dbe9567094367903ef88dff4cbaca37f797bacdfcf4db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
